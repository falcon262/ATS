<!-- 646b10a6-fc44-4a3f-8907-789d90cdb501 e6e99ea5-161e-48ab-863c-1581752539ad -->
# AI Analysis & Scoring – Technical Implementation Plan

## Decisions & Rationale

- Provider: OpenAI API (1=b)
- Pros: fastest to integrate, high-quality models (gpt-4o-mini for cost/speed), simple billing; portable prompts.  
- Cons: data residency not guaranteed; per-token cost; org compliance may need DPA.
- Resume text extraction: Local parsing (2=b) with PdfPig + DOCX parser (+ Tesseract OCR fallback)  
- Pros: no file upload to third-party; predictable cost; works offline.  
- Cons: engineering effort; OCR accuracy lower than cloud OCR; layout lost (can be mitigated with heuristics).
- Alternatives tradeoff (for future):  
- Azure OpenAI + Azure Document Intelligence → best enterprise controls, native OCR+layout, but highest setup complexity & cloud coupling.  
- Claude/Gemini → strong reasoning; but SDKs/rate limits differ; fewer enterprise knobs than Azure OpenAI.

---

## Phase 0 – Prerequisites & Access (you do)

1. OpenAI: create API key; add a credit card; note organization ID.  
2. Security: decide if prompts/logs may include PII (GDPR/DP laws).  
3. Install Tesseract OCR (Windows): `choco install tesseract` (or installer). Add to PATH.  
4. Verify sample PDFs/DOCX for testing (5–10 resumes + 2 job descriptions).

---

## Phase 1 – Data Model & Settings (server only)

- Reuse existing columns (already nullable):  
- Candidate: `AIAnalysisJson`  
- Application: `AIScore`, `AIMatchSummary`, `AIAnalysisDetailsJson`, `SkillMatchScoresJson`
- Add settings (per-tenant capable) via ABP SettingDefinition:  
- `ATS.AI.Provider = OpenAI`  
- `ATS.AI.Model = gpt-4o-mini`  
- `ATS.AI.MaxBudgetPerAnalysisCents = 3`  
- `ATS.AI.Enabled = true`
- Secrets: store `OpenAI:ApiKey` in `appsettings.secrets.json`/User Secrets/env var.

Files to touch:

- `src/ATS.Application/AI/AIOptions.cs` (POCO)  
- `src/ATS.Domain/Settings/AISettings.cs` (definitions)  
- `src/ATS.HttpApi.Host/appsettings.*.json` (bind, no secrets)  

---

## Phase 2 – Document Parsing Pipeline (server)

Create a composable extraction service:

- `IDocumentTextExtractor` + implementations:  
- `PdfPigTextExtractor` (PDF text first; if low yield, page images → OCR)  
- `DocxTextExtractor` (OpenXML or DocX)  
- `OcrTextExtractor` (Tesseract)  
- Heuristics: remove headers/footers, merge hyphenated breaks, normalize whitespace, cap at ~30k chars; compute simple sections (Education/Experience/Skills) with regex cues.

Files:

- `src/ATS.Application/AI/Parsing/IDocumentTextExtractor.cs`  
- `.../PdfPigTextExtractor.cs` (NuGet: `UglyToad.PdfPig`)  
- `.../DocxTextExtractor.cs` (NuGet: `DocumentFormat.OpenXml` or `Xceed.Words.NET`)  
- `.../OcrTextExtractor.cs` (NuGet: `Tesseract`)  
- `.../ResumeTextExtractor.cs` (orchestrator selecting strategy)

---

## Phase 3 – LLM Analysis & Scoring (server)

Define a single orchestrator service that:

- Accepts inputs: normalized resume text, structured candidate basics, job description (from `Job` entity), and optionally screening answers.
- Produces outputs (JSON):  
- `overallScore` (0–100), `hireBand` (Strong/Consider/No),  
- `skillsMatch` (list with weights), `yearsExperience`, `educationFit`, `locationFit`,  
- `explanations` (short bullet points), `riskFlags`, `followUpQuestions`.
- Prompting: system message with rubric + JSON schema; temperature 0.1; set max tokens.  
- Token control: truncate long resume text; optional two-pass (extract->score).  
- Cost guard: refuse analysis when estimated token cost exceeds budget.

Files:

- `src/ATS.Application/AI/OpenAI/OpenAiClientFactory.cs` (NuGet: `OpenAI` official SDK)  
- `src/ATS.Application/AI/Prompts/ScoringRubric.md` (checked-in prompt)  
- `src/ATS.Application/AI/Analysis/AiScoringService.cs` (core orchestrator)

---

## Phase 4 – Integration & Background Processing (server)

- Trigger points:  
- On public application submit (existing `PublicJobAppService`): enqueue analysis job.  
- On candidate resume upload: enqueue (light analysis for candidate profile).
- Background job: ABP `IBackgroundJobManager` (or Hangfire if installed later).  
- Job payload: `ApplicationId` (or `CandidateId`); idempotent; retries with jitter; store errors.
- Persistence: update `Applications` with `AIScore`, `AIMatchSummary`, details JSON; update `Candidates.AIAnalysisJson` snapshot.
- Caching: hash (jobId + candidate resume hash + job description hash) to skip unchanged.

Files:

- `src/ATS.Application/AI/Jobs/AnalyzeApplicationJob.cs`  
- Subscribe in `PublicJobAppService.SubmitApplicationAsync` and `CandidateAppService.UploadResumeAsync`.

---

## Phase 5 – HTTP API & Angular UI

Server endpoints (IAIAnalysisAppService already exists – reuse/extend):

- `POST /api/app/ai/applications/{id}/analyze` → enqueue + 202  
- `GET /api/app/ai/applications/{id}` → returns latest analysis DTO  
- `POST /api/app/ai/candidates/{id}/reanalyze` (optional)

Angular (read-only by permissions):

- Application Detail: card with `AIScore`, `Hire band`, top 5 skill matches, summary; "Analyze now" button (admin only).  
- Candidate Detail: smaller analysis badge + latest summary; rerun if permitted.  
- List views: colored score chips; filter by score or hire band.  
- Loading & error toasts; pending state while background job runs.

Files:

- `angular/src/app/features/applications/application-detail/*`  
- `angular/src/app/features/candidates/candidate-detail/*`  
- `angular/src/app/proxy/ai/*` (proxy after API)

---

## Phase 6 – Backfill & Ops

- Backfill command (console or DbMigrator host) to process existing applications in batches of 50; exponential backoff; nightly cron optional.  
- Admin page: AI settings (model, budget, enable/disable), plus logs viewer (last 100 analyses).  
- Export CSV of scores.

Files:

- `src/ATS.DbMigrator/Backfill/AnalyzeExistingApplicationsCommand.cs`

---

## Phase 7 – Observability, Safety & Cost Controls

- Logging: prompt+response token counts (not full prompts when PII-restricted).  
- Metrics: success rate, avg latency, cost per analysis (approx).  
- Rate-limits: simple semaphore (concurrency=2–4); retry on 429 with backoff.  
- Safety: redact emails/phones before sending; configurable.  
- Tests:  
- Unit tests for parsers (PDF/DOCX), heuristics, scoring normalization.  
- Integration tests with an LLM stub (deterministic fake).  
- Feature flag: `ATS.AI.Enabled` guards all triggers.

---

## Deliverables & Milestones

- M1 (Day 1–2): Phase 1–2 complete; sample extraction works for PDFs/DOCX.  
- M2 (Day 3–4): LLM scoring service + JSON schema; manual API to analyze.  
- M3 (Day 5): Background jobs wired; app submit triggers analysis; persistence done.  
- M4 (Day 6): Angular UI surfacing + filters; role-permission checks.  
- M5 (Day 7): Backfill job + ops dashboards; documentation.

---

## Your Setup Checklist (step-by-step)

1. OpenAI

- Create key: https://platform.openai.com  
- Copy API Key and (if applicable) Org ID.  
- Add to backend: `dotnet user-secrets set OpenAI:ApiKey "***"` in `ATS.HttpApi.Host`.

2. Local Parsing

- Install Tesseract: Windows MSI or `choco install tesseract`.
- Validate `tesseract --version` works in PATH.

3. NuGet packages (we will add during implementation)

- `OpenAI` (official), `UglyToad.PdfPig`, `DocumentFormat.OpenXml` (or `Xceed.Words.NET`), `Tesseract`, `SixLabors.ImageSharp`.

4. Cost/limits

- Set `ATS.AI.MaxBudgetPerAnalysisCents` (start at 3–5 cents).  
- Choose model `gpt-4o-mini`.

---

## Acceptance Criteria

- Auto-analysis runs within 10–20s for a typical resume+JD; persisted results available in application detail.  
- Deterministic JSON contract with versioned schema; score (0–100) and band present.  
- Re-analysis guarded by permissions and budget; idempotent on unchanged inputs.  
- Works for PDF and DOCX resumes; OCR fallback for image-based PDFs.  
- No secrets in logs; configurable redaction on prompts.

### To-dos

- [ ] Add AI settings (provider/model/budget/enabled) and bind OpenAI API key
- [ ] Implement PdfPig/Docx/Tesseract text extraction orchestrator
- [ ] Build LLM scoring orchestrator with rubric and JSON schema
- [ ] Enqueue analysis on application submit/resume upload; process job
- [ ] Expose analyze/reanalyze endpoints and result queries
- [ ] Surface score/band in Application & Candidate detail; add filters
- [ ] Implement batch backfill job for historical applications
- [ ] Add logging/metrics, cost guards, redaction, and tests